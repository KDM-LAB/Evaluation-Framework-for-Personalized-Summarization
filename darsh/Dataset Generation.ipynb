{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainfilename = '../../data/train.tsv'\n",
    "# validfilename = '../../data/valid.tsv'\n",
    "# testfilename = '../../data/personalized_test.tsv'\n",
    "docsfilename = './news.tsv'\n",
    "testfilename = './personalized_test.tsv'\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD_FREQ_THRESHOLD = 3\n",
    "# MAX_CONTENT_LEN = 500\n",
    "# MAX_BODY_LEN = 100\n",
    "# MAX_TITLE_LEN = 16\n",
    "# WORD_EMBEDDING_DIM = 300\n",
    "MAX_CLICK_LEN = 50\n",
    "\n",
    "# word2freq = {}\n",
    "# word2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    pat = re.compile(r'[\\w]+|[.,!?;|]')\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit\n",
    "news_id_hl = {}\n",
    "\n",
    "def read_news(filename,filer_num=3):\n",
    "    news={}\n",
    "    \n",
    "    category, subcategory=[], []\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_cnt=Counter()\n",
    "    err = 0\n",
    "    news_data = pd.read_csv(filename, sep='\\t')\n",
    "    news_data.fillna(value=\" \", inplace=True)\n",
    "    for i in tqdm(range(len(news_data))):\n",
    "        doc_id,vert,_, title, snipplet= news_data.loc[i,:][:5]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "        \n",
    "        #edit\n",
    "        news_id_hl[doc_id] = title\n",
    "        \n",
    "        title = title.lower()\n",
    "        title = word_tokenize(title)\n",
    "        snipplet = snipplet.lower()\n",
    "        snipplet = word_tokenize(snipplet)\n",
    "        category.append(vert)\n",
    "        news[doc_id] = [vert,title,snipplet]     \n",
    "        word_cnt.update(snipplet+title)\n",
    "    # 0: pad; 1: <sos>; 2: <eos>\n",
    "    word = [k for k , v in word_cnt.items() if v >= filer_num]\n",
    "    word_dict = {k:v for k, v in zip(word, range(3,len(word)+3))}\n",
    "    category=list(set(category))\n",
    "    category_dict={k:v for k, v in zip(category, range(1,len(category)+1))}\n",
    "\n",
    "    return news,news_index,category_dict,word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc2ID(doclist,news2id):\n",
    "    return [news2id[i] for i in doclist if i in news2id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PadDoc(doclist):\n",
    "    if len(doclist) >= MAX_CLICK_LEN:\n",
    "        return doclist[-MAX_CLICK_LEN:]\n",
    "    else:\n",
    "        return [0] * (MAX_CLICK_LEN-len(doclist)) + doclist[:MAX_CLICK_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 113762/113762 [00:57<00:00, 1971.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 11.4 s, total: 1min 35s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%time news,news_index,category_dict,word_dict = read_news(docsfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_user(filename,news_index):\n",
    "        \n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    \n",
    "    df['clicknewsID'] = df['clicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(','),news_index)))\n",
    "    \n",
    "    df['posnewID']  = df['posnewID'].apply(lambda x: Doc2ID(x.split(','),news_index))\n",
    "    \n",
    "    df['rewrite_titles'] = df['rewrite_titles'].apply(lambda x: [i.lower() for i in x.split(';;')] )\n",
    "    \n",
    "    pos_lists = []\n",
    "    for userindex, (pos_lis, rewrite_title_lis) in enumerate(zip(df['posnewID'].values.tolist(), df['rewrite_titles'].values.tolist())):\n",
    "        for pos, rewrite_title in zip(pos_lis, rewrite_title_lis):\n",
    "            if rewrite_title.strip() == '':\n",
    "                continue\n",
    "            else:\n",
    "                pos_lists.append([userindex, pos, rewrite_title])\n",
    "    \n",
    "    return df['clicknewsID'].values.tolist(), pos_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 524 ms, total: 2.49 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%time TestUsers, TestSamples = parse_test_user(testfilename,news_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition of model generated summaries to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_NRMS.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 1 for pens nrms\n",
    "model_id = 1\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_NAML.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "# print(model_summ)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 1 for pens naml\n",
    "model_id = 2\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_EBNR.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "# print(model_summ)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 3 for pens ebnr\n",
    "model_id = 3\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_NRMS_2.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "# print(model_summ)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 3 for pens ebnr\n",
    "model_id = 4\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hyps_NAML_2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# opening the file in read mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m my_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyps_NAML_2.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# reading the file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m my_file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.conda/envs/newenv1.2.2/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hyps_NAML_2.txt'"
     ]
    }
   ],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_NAML_2.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "# print(model_summ)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 3 for pens ebnr\n",
    "model_id = 5\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"hyps_EBNR_2.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "model_summ = data.split(\"\\n\")\n",
    "# print(model_summ)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_dict = []\n",
    "\n",
    "key_list = list(news_index.keys())\n",
    "# print(key_list)\n",
    "val_list = list(news_index.values())\n",
    "\n",
    "# print key with val 100\n",
    "position = val_list.index(100)\n",
    "\n",
    "#model summary index\n",
    "idx = 0\n",
    "#model_id = 3 for pens ebnr\n",
    "model_id = 6\n",
    "\n",
    "for sample in TestSamples:\n",
    "    dic = {}\n",
    "    uid = \"NT\"+str(sample[0]+1)\n",
    "    doc_idx = sample[1]\n",
    "#     news_id_idx = val_list.index(doc_idx)\n",
    "#     print(news_id_idx)\n",
    "    position = val_list.index(sample[1])\n",
    "    news_id_idx = key_list[position]\n",
    "#     print(news_id_idx)\n",
    "    grf_summ = sample[2]\n",
    "#     original_hl = news[news_id_idx]\n",
    "    \n",
    "    dic = {'user_id': uid, 'doc_id': news_id_idx, 'user_summ': grf_summ, 'doc_summ': news_id_hl[news_id_idx], \n",
    "           'model_id': model_id, 'model_'+str(model_id)+'_summ': model_summ[idx]}\n",
    "    idx = idx + 1\n",
    "    \n",
    "    setup_dict.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = setup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "for sample in datalist:\n",
    "    doc_id = sample['doc_id']\n",
    "    if sample['model_id'] == 1:\n",
    "        if doc_id in dataset.keys() and ('1' in dataset[doc_id]['m_dict'].keys()):    \n",
    "            dataset[doc_id]['u_dict'].update({sample['user_id']: sample['user_summ']})\n",
    "            dataset[doc_id]['m_dict']['1'].update({sample['user_id']: sample['model_1_summ']})\n",
    "            dataset[doc_id]['m_dict']['2'].update({sample['user_id']: ' '})\n",
    "            dataset[doc_id]['m_dict']['3'].update({sample['user_id']: ' '})\n",
    "            dataset[doc_id]['m_dict']['4'].update({sample['user_id']: ' '})\n",
    "#             dataset[doc_id]['m_dict']['5'].update({sample['user_id']: ' '})\n",
    "            dataset[doc_id]['m_dict']['6'].update({sample['user_id']: ' '})\n",
    "        else:\n",
    "            dic = {}\n",
    "            u_dict, m_dict = {}, {}\n",
    "            u_dict[sample['user_id']] = sample['user_summ']\n",
    "            model_dict = {}\n",
    "            m_dict[sample['user_id']] = sample['model_1_summ']\n",
    "\n",
    "            model_dict.update({'1': m_dict, '2':{}, '3': {}, '4':{}, '6': {}})\n",
    "            dic = {'doc_text': news[doc_id], 'doc_summ': sample['doc_summ'], 'u_dict': u_dict, 'm_dict': model_dict}    \n",
    "            dataset[doc_id] = dic\n",
    "    \n",
    "    elif sample['model_id'] == 2:\n",
    "            \n",
    "            dataset[doc_id]['m_dict']['2'].update({sample['user_id']: sample['model_2_summ']})\n",
    "\n",
    "    elif sample['model_id'] == 3 :\n",
    "            dataset[doc_id]['m_dict']['3'].update({sample['user_id']: sample['model_3_summ']})\n",
    "    \n",
    "    elif sample['model_id'] == 4 :\n",
    "            dataset[doc_id]['m_dict']['4'].update({sample['user_id']: sample['model_4_summ']})\n",
    "    \n",
    "#     elif sample['model_id'] == 5 :\n",
    "#             dataset[doc_id]['m_dict']['5'].update({sample['user_id']: sample['model_5_summ']})\n",
    "    \n",
    "    elif sample['model_id'] == 6 :\n",
    "            dataset[doc_id]['m_dict']['6'].update({sample['user_id']: sample['model_6_summ']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ea14d0bd9c7a0f4f2d255c0662c7e1119328c505d1c17d9d8f159415dfcf69"
  },
  "kernelspec": {
   "display_name": "newenv1.2.2",
   "language": "python",
   "name": "newenv1.2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
